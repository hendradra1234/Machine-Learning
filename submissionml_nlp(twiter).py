# -*- coding: utf-8 -*-
"""SubMissionML_NLP(Twiter).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B1OnTuIYrqe6OX-yaAO6Q6Nz3wOkV33I
"""

import os
from google.colab import drive
drive.mount('/content/gdrive')
os.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/My Drive/Kaggle'

filename = 'twitter-and-reddit-sentimental-analysis-dataset.zip'
exfile = 'twiterredditsentimentanalyis'

cd /content/gdrive/My Drive/Kaggle

!kaggle datasets download -d cosmos98/twitter-and-reddit-sentimental-analysis-dataset

import os
import zipfile as zf

try:
  extFile = zf.ZipFile(filename)
  extFile.extractall(exfile)
  extFile.close()
  print('Dekompresi dataset sukses')
except(FileExistsError,FileNotFoundError):
  print('Tidak dapat melakukan decompresi dataset')
finally:
  print('Operation terminated')

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

vocabSize = 5000
maxlength = 100
testSize = 0.2
embeddedDim = 64
batchSize = 128
trunc_type = 'post'
padding_type = 'post'
oovTok = '<OOV>'
files = os.listdir(exfile)
datasetFiles = os.path.join(exfile,files[1])
print('file dataset {}'.format(datasetFiles))

import json as js
import pandas as pd
normalizationDataset = pd.read_csv(datasetFiles)
normalizationDataset.head()

review = normalizationDataset['clean_text']
rating = []

for r in normalizationDataset['category']:
  if r == -1.0:
    rating.append('negative')
  if r == 0.0:
    rating.append('neutral')
  if r == 1.0:
    rating.append('positive')

  if r != -1.0 and r != 0.0 and r != 1.0:
    rating.append('negative')

normalizationDataset = pd.DataFrame({'tweet':review,'category':rating})
normalizationDataset.head()

import matplotlib.pyplot as plt

sizes = normalizationDataset.category.value_counts().values
labels = normalizationDataset.category.value_counts().index

plt.pie(
    sizes,
    labels=labels,
    autopct='%.1f%%',
    shadow=True,
    pctdistance=0.85,
    labeldistance=1.05,
    startangle=20,
    explode = [0 if 1 > 0 else 0.2 for i in range(len(sizes))])

plt.axis('equal')
plt.show()
print('total sample dataset: {} unit'.format(sum(sizes)))

normalizationDataset.shape

normalizationDataset.category.value_counts()

lenDataset = normalizationDataset.groupby('category')
classQty = lenDataset.ngroups
print('total class {}'.format(classQty))
category = lenDataset['category']
category = category
lenDataset.size()

normalizationDataset.head()

#mormalization dataset
import re

def cleanLowerCase(textDataset):
  return str(textDataset).lower()

normalizationDataset['tweet'] = normalizationDataset['tweet'].apply(cleanLowerCase)
normalizationDataset['category'] = normalizationDataset['category'].apply(cleanLowerCase)

normalizationDataset.head()

labelCollm = normalizationDataset.category
labelCollm = labelCollm.unique()
print('Class {}'.format(labelCollm))

category = pd.get_dummies(normalizationDataset.category)
hotEncodingDataset = pd.concat([normalizationDataset,category],axis=1)
hotEncodingDataset = hotEncodingDataset.drop(columns='category')
hotEncodingDataset.head()

hotEncodingDataset.shape

#Data split
dataHeadlineDescription = hotEncodingDataset['tweet'].values
label = hotEncodingDataset[labelCollm].values
dataHeadlineDescription_train,dataHeadlineDescription_test,label_train,label_test = train_test_split(dataHeadlineDescription,label,test_size=testSize)
print('Training Shape {}'.format(dataHeadlineDescription_train.shape))
print('Training Test shape  {}'.format(dataHeadlineDescription_test.shape))
print('Training label shape {}'.format(label_train.shape))
print('test label shape {}'.format(label_test.shape))

#Tokenizer 
tokenizer = Tokenizer(num_words=vocabSize,oov_token=oovTok)


tokenizer.fit_on_texts(dataHeadlineDescription_train)
tokenizer.fit_on_texts(dataHeadlineDescription_test)

trainSequences = tokenizer.texts_to_sequences(dataHeadlineDescription_train)
testSequences = tokenizer.texts_to_sequences(dataHeadlineDescription_test)


trainPadded = pad_sequences(
                            trainSequences,
                            maxlen=maxlength,
                            padding=padding_type,
                            truncating=trunc_type
    )
testpadded = pad_sequences(
                           testSequences,
                           maxlen=maxlength,
                           padding=padding_type,
                           truncating=trunc_type
    )

idx = len(tokenizer.word_index)
print('{} unique token'.format(idx))

print('training Padding shape {}'.format(trainPadded.shape))
print('test Padding shape {}'.format(testpadded.shape))

#Creating Model
from tensorflow import keras
from tensorflow.keras.layers import Embedding,Dense,Conv1D,LSTM,Flatten,GlobalAveragePooling1D,Bidirectional,Dropout
from tensorflow.keras.optimizers import Adam



model = keras.models.Sequential()
model.add(Embedding(idx+1,embeddedDim,input_length=maxlength))
model.add(Bidirectional(LSTM(100,return_sequences=True)))
model.add(Bidirectional(LSTM(100,return_sequences=True)))
model.add(Bidirectional(LSTM(64,return_sequences=True)))
model.add(Bidirectional(LSTM(32)))
model.add(Flatten())
model.add(Dropout(0.5))
model.add(Dense(1024,activation='relu'))
model.add(Dense(embeddedDim,activation='relu'))
model.add(Dense(3,activation='softmax'))

optimizer = Adam(lr=0.001)
model.compile(loss='categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])
model.summary()

#Fit model

ACC_TRESHOLD = 0.96
class Callback(tf.keras.callbacks.Callback):
  def on_epoch_end(self,epoch,logs={}):
    if (logs.get('accuracy') > ACC_TRESHOLD):     
      print('\n{}% accuracy reached,end the Training\n'.format(ACC_TRESHOLD*100))
      print('\nActual accuracy {}%\n'.format(logs.get('accuracy')*100))
      self.model.stop_training = True
     
checkpoint_filepath = '/tmp/NLPTwiter/checkpoint/TwiterModelSentimentAnalisis/'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='accuracy',
    mode='max',
    save_best_only=True)
saveLog =  tf.keras.callbacks.CSVLogger('/tmp/NLPTwiter/trainingTwiterLog.csv',separator=',',append=True)

callbackStop = Callback()

with tf.device('/device:GPU:0'):
  history = model.fit(
    trainPadded,
    label_train,
    epochs=100,
    validation_data = (testpadded,label_test),
    steps_per_epoch=100,
    batch_size = batchSize,
    use_multiprocessing=True,
    validation_steps=50,
    callbacks=[callbackStop,model_checkpoint_callback,saveLog]
    )
  
  model.save('/tmp/NLPTwiter/CategoryModel_3ClassTwiter.h5')
  print('\nTraining Selesai')

#plotting
import matplotlib.pyplot as plt

loss = history.history['loss']
val_loss = history.history['val_loss']
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
epoch = range(len(acc))

plt.plot(epoch,acc,label='accuracy')
plt.plot(epoch,loss,label='training loss')
plt.xlabel('Epoch')
plt.ylabel('accuracy')
plt.title('Training Accuracy and Loss')
plt.legend()
plt.show()
plt.plot(epoch,val_acc,label='validation accuracy')
plt.plot(epoch,val_loss,label='validation loss')
plt.xlabel('Epoch')
plt.ylabel('accuracy')
plt.title('Validation Acuracy and Loss')
plt.legend()
plt.show()

def Predict(textPredict):
  paddedtext = pad_sequences(tokenizer.texts_to_sequences([textPredict]),maxlen=maxlength,truncating=trunc_type,padding=padding_type)

  prediction = model.predict(paddedtext,batch_size=batchSize)
  label_encoder = preprocessing.LabelEncoder()

  if (prediction[0,0] > prediction[0,1]) and (prediction[0,0] > prediction[0,2]):
    idx_p = 0
  elif (prediction[0,1] > prediction[0,0]) and (prediction[0,1] > prediction[0,2]):
    idx_p = 1
  elif (prediction[0,2] > prediction[0,0]) and (prediction[0,2] > prediction[0,1]):
    idx_p = 2
  print('\nTweet: {}'.format(textPredict))
  print('Sentiment : {} \n'.format(labelCollm[idx_p]))

#Prediction
from sklearn import preprocessing
text_1 = 'answer who among these the most powerful world i been seen'
text_2 = 'whats its is so bad i does want it its a trast phone'
text_3 = 'not bad this phone pretty good'
text_4 = 'hi buddy you looks so handsome i like you babe'
Predict(text_1)
Predict(text_2)
Predict(text_3)
Predict(text_4)